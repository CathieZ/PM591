---
title: "PM 591 -- Assignment 3."
author: "chuhan zhang"
date: "Due 2/28/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
```
<br>


Exercise 1

a) Build a KNN classifier to predict stroke using the ischemic stroke data and tune the complexity parameter $\,K=1,\ldots,50$ using a single-split validation set. As features use "sex", "age", "CoronaryArteryDisease", "MaxStenosisByDiameter" and "MATXVolProp"). Plot the classification error as a function of $\,K$. Which value of $K$ do you choose? Explain. 
```{r}
stroke <- read.csv("stroke.csv")

stroke$Stroke                      <- factor(stroke$Stroke, levels=c('N', 'Y'), labels=c("No", "Yes"))
stroke$NASCET                      <- factor(stroke$NASCET, labels=c("No", "Yes"))
stroke$sex                         <- factor(stroke$sex, labels=c("Female", "Male"))
stroke$SmokingHistory              <- factor(stroke$SmokingHistory, labels=c("No", "Yes"))
stroke$AtrialFibrillation          <- factor(stroke$AtrialFibrillation, labels=c("No", "Yes"))
stroke$CoronaryArteryDisease       <- factor(stroke$CoronaryArteryDisease, labels=c("No", "Yes"))
stroke$DiabetesHistory             <- factor(stroke$DiabetesHistory, labels=c("No", "Yes"))
stroke$HypercholesterolemiaHistory <- factor(stroke$HypercholesterolemiaHistory, labels=c("No", "Yes"))
stroke$HypertensionHistory         <- factor(stroke$HypertensionHistory, labels=c("No", "Yes"))
```

```{r stratified splitting}
set.seed(303)
n = nrow(stroke)
positives = (1:n)[stroke$Stroke=='Yes']
negatives = (1:n)[stroke$Stroke=='No']

positives_train = sample(positives, floor(0.7*length(positives)))
positives_val = setdiff(positives, positives_train)

negatives_train = sample(negatives, floor(0.7*length(negatives)))
negatives_val = setdiff(negatives, negatives_train)

rowstrain = c(positives_train, negatives_train)
rowsval = c(positives_val, negatives_val)

stroke_train = stroke[c(positives_train, negatives_train), ]
stroke_val = stroke[c(positives_val, negatives_val), ]

ntrain = nrow(stroke_train); nval=nrow(stroke_val)

table(stroke_train$Stroke)
table(stroke_val$Stroke)
```

```{r, cache=T}
stroke.tsk  <- as_task_classif(stroke,
                              target = "Stroke",
                              positive = "Yes",
                              id = "Ischemic Stroke")
stroke.tsk$select(c("sex", "age", "CoronaryArteryDisease", "MaxStenosisByDiameter", "MATXVolProp"))
print(stroke.tsk)
```

```{r}
require(mlr3learners)
require(kknn)       
stroke.lrn <- lrn("classif.kknn", k = 1)
stroke.lrn$train(stroke.tsk, row_ids = rowstrain)

suppressMessages(library(MASS))
stroke_sensitivity <- numeric(50)
stroke_specificity <- numeric(50)
storke_acc <- numeric(50)
stroke_ce <- numeric(50)

library(mlr3learners) 
library(kknn) 
suppressMessages(library(MASS))

stroke_sensitivity <- numeric(50)
stroke_specificity <- numeric(50)
stroke_ce          <- numeric(50)
stroke_acc         <- numeric(50)

for (i in 1:50) {
  stroke.lrn <- lrn("classif.kknn", k = i)
  
  stroke.lrn$train(stroke.tsk, row_ids=rowstrain)
  
  stroke_predict <- stroke.lrn$predict(stroke.tsk, row_ids = rowsval)
  stroke_sensitivity[i] <- stroke_predict$score(msr("classif.sensitivity"))
  stroke_specificity[i] <- stroke_predict$score(msr("classif.specificity"))
  stroke_ce[i]  <- stroke_predict$score(msr("classif.ce"))
  stroke_acc[i] <- stroke_predict$score(msr("classif.acc"))
}

k = c(1:50)
plot(x = k, y = stroke_ce, 
     main = "KNN Classification Error of Ischemic Stroke", xlab = "K", ylab = "Classification Error")
  
```

Conclusion: I will choose k=13, because the k is max value in the flattening curve and we need to keep the classification error value .


b) Repeat a) 9 additional times with different random training/validation splits (use a loop). Plot the 10 curves, analogs to the one obtained in a. in the same graph. Do you choose the same value of $\,K$ for each of the 10 splits? What does this say about the stability/variability of using a single training/validation split to perform model selection? 
```{r}
sensitivity <- matrix(nrow=50,ncol=9)
specificity <- matrix(nrow=50, ncol=9)
ce <- matrix(nrow=50, ncol=9)
acc <- matrix(nrow=50, ncol=9)
for(i in 1:9){
  positives_train = sample(positives,floor(0.7*length(positives)))
  positives_val = setdiff(positives,positives_train)
  rowstrain = c(positives_train,negatives_train)
  rowsval = c(positives_val, negatives_val)
  stroke_train = stroke[c(positives_train, negatives_train),]
  stroke_val = stroke[c(positives_val, negatives_val),]
  for (h in 1:50){
    stroke.lrn <-lrn("classif.kknn",k = h)
    stroke.lrn$train(stroke.tsk, row_ids = rowstrain)
    stroke_predict <- stroke.lrn$predict(stroke.tsk, row_ids = rowsval)
    sensitivity[h,i] <- stroke_predict$score(msr("classif.sensitivity"))
    specificity[h,i] <- stroke_predict$score(msr("classif.specificity"))
    ce[h,i] <- stroke_predict$score(msr("classif.ce"))
    acc[h,i] <- stroke_predict$score(msr("classif.acc"))
  }
}
```

```{r}
suppressMessages(library(ggplot2))
ce_10 <- data.frame(ce, stroke_ce, k)
plot <- ggplot(data = ce_10) +
  geom_point(mapping = aes(x=k, y=stroke_ce, color="No.1"))+
  geom_point(mapping = aes(x=k, y=X1, color="No.2"))+
  geom_point(mapping = aes(x=k, y=X2, color="No.3"))+
  geom_point(mapping = aes(x=k, y=X3, color="No.4"))+
  geom_point(mapping = aes(x=k, y=X4, color="No.5"))+
  geom_point(mapping = aes(x=k, y=X5, color="No.6"))+
  geom_point(mapping = aes(x=k, y=X6, color="No.7"))+
  geom_point(mapping = aes(x=k, y=X7, color="No.8"))+
  geom_point(mapping = aes(x=k, y=X8, color="No.9"))+
  geom_point(mapping = aes(x=k, y=X9, color="No.10"))+
  labs(x="k", y ="classfication error")
plot
```
Conclusion: For the different split, the k were different so i think it is not stable for this split and training.


c) Now tune the complexity parameter $\,K=1,\ldots,50$ using now 5-fold cross-validation instead of a single training/validation split. Which value of $k$ do you choose? Explain. 
```{r}
set.seed(202)
lgr::get_logger("mlr3")$set_threshold("warn")    
lgr::get_logger("bbotk")$set_threshold("warn")
stroke.cv_ce <- numeric(50)
for (i in 1:50){
  knn.lrn <- lrn("classif.kknn", k=i)
  cv <- rsmp("cv", folds=5)
  stroke.rr <- resample(stroke.tsk, knn.lrn, cv, store_models = TRUE)
  stroke.cv_ce[i] <- stroke.rr$aggregate(msr("classif.ce"))
}
  plot(x = k, y = stroke.cv_ce,
        main="KNN Classification Error of Ischemic Stroke using 5 folds cross validation", xlab = "K", ylab = "Classification Error")
```
Conclusion: I will choose the k=10 since it provides the lowest classification error.

d) Repeat c) 9 additional times with different cross-validation splits (use a loop). Plot the 10 curves, analogs to the one obtained in c. in the same graph. Do you choose the same value of $\,K$ for each of the 10 splits? What does this say about the stability/variability of using cross/validation to perform model selection compared to a single split? 
```{r}
ce2 <- matrix(nrow=50, ncol=9)
for (i in 1:9){
  positives_train = sample(positives, floor(0.7*length(positives)))
  positives_val = setdiff(positives, positives_train)
  
  negatives_train = sample(negatives, floor(0.7*length(negatives)))
  negatives_val = setdiff(negatives, negatives_train)
  
    rowstrain = c(positives_train, negatives_train)
  rowsval = c(positives_val, negatives_val)

  stroke_train = stroke[c(positives_train, negatives_train), ]
  stroke_val = stroke[c(positives_val, negatives_val), ]
for (h in 1:50){
  knn.lrn <- lrn("classif.kknn", k=h)
  cv <- rsmp("cv", folds = 5)
  stroke.rr <- resample(stroke.tsk, knn.lrn, cv, store_models = TRUE)
    ce2[h,i]  <- stroke.rr$aggregate(msr("classif.ce"))
}
}
```

```{r}
ce2_10 <- data.frame(ce2, stroke.cv_ce, k)
plot2 <- ggplot(data = ce2_10)+
  geom_point(mapping = aes(x = k, y = X1, color="NO.2"))+
  geom_point(mapping = aes(x = k, y = X2, color="NO.3"))+
  geom_point(mapping = aes(x = k, y = X3, color="NO.4"))+
  geom_point(mapping = aes(x = k, y = X4, color="NO.5"))+
  geom_point(mapping = aes(x = k, y = X5, color="NO.6"))+
  geom_point(mapping = aes(x = k, y = X6, color="NO.7"))+
  geom_point(mapping = aes(x = k, y = X7, color="NO.8"))+
  geom_point(mapping = aes(x = k, y = X8, color="NO.9"))+
  geom_point(mapping = aes(x = k, y = X9, color="NO.10"))+
  geom_point(mapping = aes(x = k, y = stroke.cv_ce, color="NO.1"))+
  labs(x = "k", y = "classification error")
plot2
```


Exercise 2.

Using the ischemic stroke data with the same features than in exercise 1, train an evaluate the performance of an LDA, QDA, and logistic regression classifiers using the mlr3 package. Plot the ROC curve and report the AUC for each of the classifiers. Compare the performance of the three classifiers. Which one would you choose for predicting stroke?
```{r}
set.seed(303)
n = nrow(stroke)
positives = (1:n)[stroke$Stroke=='Yes']
negatives = (1:n)[stroke$Stroke=='No']

positives_train = sample(positives, floor(0.7*length(positives)))
positives_val = setdiff(positives, positives_train)

negatives_train = sample(negatives, floor(0.7*length(negatives)))
negatives_val = setdiff(negatives, negatives_train)

rowstrain = c(positives_train, negatives_train)
rowsval = c(positives_val, negatives_val)

stroke_train = stroke[c(positives_train, negatives_train), ]
stroke_val = stroke[c(positives_val, negatives_val), ]

ntrain = nrow(stroke_train); nval=nrow(stroke_val)
```

```{r}
stroke.tsk  <- as_task_classif(stroke,
                              target = "Stroke",
                              positive = "Yes",
                              id = "Ischemic Stroke") 

stroke.tsk$select(c("sex", "age", "CoronaryArteryDisease", "MaxStenosisByDiameter", "MATXVolProp"))
```

```{r}
library(mlr3viz)
library(GGally)
stroke.lrn1 <- lrn("classif.lda", predict_type = "prob")
stroke.lrn1$train(stroke.tsk, row_ids=rowstrain)
stroke_predict1 <- stroke.lrn1$predict(stroke.tsk, row_ids = rowsval)
autoplot(stroke_predict1, type="roc")

stroke.lrn2 <- lrn("classif.qda", predict_type = "prob")
stroke.lrn2$train(stroke.tsk, row_ids=rowstrain)
stroke_predict2 <- stroke.lrn2$predict(stroke.tsk, row_ids = rowsval)
autoplot(stroke_predict2, type="roc")

stroke.lrn3 <- lrn("classif.log_reg", predict_type = "prob")
stroke.lrn3$train(stroke.tsk, row_ids=rowstrain)
stroke_predict3 <- stroke.lrn3$predict(stroke.tsk, row_ids = rowsval)
autoplot(stroke_predict3, type="roc")
```

```{r}
auc_lda <- stroke_predict1$score(msr("classif.auc"))
auc_qda <- stroke_predict2$score(msr("classif.auc"))
auc_log <- stroke_predict3$score(msr("classif.auc"))
auc <- data.frame(auc_lda, auc_qda, auc_log)
auc
```

Conclusion: I will choose logistic regression model because it has the highest value of auc.